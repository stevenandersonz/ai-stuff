{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"./names.txt\", 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n"
     ]
    }
   ],
   "source": [
    "print(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "stoi = {l:i for i,l in enumerate('.' + string.ascii_lowercase)}\n",
    "itos = {l:i for i,l in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import float32\n",
    "\n",
    "\n",
    "def create_dataset(words):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for name in words:\n",
    "        name =name\n",
    "        for i,letter in enumerate(name[:len(name)-1]):\n",
    "            x_i = [0] * 27\n",
    "            y_i = [0] * 27\n",
    "            letter_idx = stoi[letter]\n",
    "            target_idx = stoi[name[i+1]]\n",
    "            x_i[letter_idx] = 1\n",
    "            y_i[target_idx] = 1\n",
    "            X.append(x_i)\n",
    "            Y.append(y_i)\n",
    "    X = torch.tensor(X, dtype=float32).to(\"cuda\")\n",
    "    print(X.shape)\n",
    "    Y = torch.tensor(Y, dtype=float32).to(\"cuda\")\n",
    "    print(Y.shape)\n",
    "    return X, Y\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51460, 27])\n",
      "torch.Size([51460, 27])\n",
      "torch.Size([16249, 27])\n",
      "torch.Size([16249, 27])\n",
      "torch.Size([16458, 27])\n",
      "torch.Size([16458, 27])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "x_tr, y_tr = create_dataset(names[:10000])\n",
    "x_dev, y_dev= create_dataset(names[n1:n2])\n",
    "x_te, y_te = create_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 4\n",
    "lr = 0.1\n",
    "batch_size = round((len(x_tr) /seq_length)+0.5) # = math.ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=100, out_features=27, bias=True)"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(27, hidden_size)\n",
    "rnn.cuda()\n",
    "linear1 = nn.Linear(hidden_size, 27)\n",
    "linear1.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import linear\n",
    "\n",
    "\n",
    "Wxh = rnn.weight_ih_l0.clone().detach().cuda()\n",
    "Whh = rnn.weight_hh_l0.clone().detach().cuda()\n",
    "bh = rnn.bias_hh_l0.clone().detach().cuda() \n",
    "bx = rnn.bias_ih_l0.clone().detach().cuda() \n",
    "\n",
    "Why = linear1.weight.clone().detach().cuda()\n",
    "by = linear1.bias.clone().detach().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3545, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out, h_n = rnn(x_tr[0].unsqueeze(0))\n",
    "logits = linear1(out)\n",
    "loss = F.cross_entropy(logits, y_tr[0].unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.retain_grad()\n",
    "for p in rnn.parameters():\n",
    "    p.retain_grad()\n",
    "\n",
    "for p in linear1.parameters():\n",
    "    p.retain_grad()\n",
    "loss.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "weight_hh_l0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "bias_ih_l0\n",
      "tensor([ 0.0780, -0.0220, -0.0328,  0.0320,  0.0151, -0.0860,  0.0567,  0.0668,\n",
      "         0.1011,  0.0412, -0.0727, -0.0556, -0.0545, -0.0491, -0.0220, -0.0551,\n",
      "         0.0400, -0.0831, -0.1060,  0.0521,  0.0686,  0.1019,  0.0633,  0.0219,\n",
      "         0.0310, -0.0883,  0.0541,  0.0714, -0.0706,  0.0204, -0.0187,  0.0064,\n",
      "         0.0407, -0.0594,  0.0012,  0.0754,  0.0340,  0.0415,  0.0702, -0.0703,\n",
      "         0.0810, -0.0325, -0.0099, -0.0586, -0.0669, -0.0349, -0.0687, -0.0479,\n",
      "        -0.0575, -0.0219,  0.0798, -0.0607, -0.0143,  0.0919, -0.0216,  0.0124,\n",
      "         0.0685,  0.0412,  0.0530,  0.0324, -0.0712,  0.0408,  0.0187,  0.0231,\n",
      "        -0.0943,  0.0050,  0.0754,  0.0662, -0.0457, -0.0708, -0.0056, -0.0155,\n",
      "         0.0488, -0.0651, -0.0302,  0.0154, -0.0608,  0.0194,  0.0743,  0.0071,\n",
      "        -0.0531,  0.0217, -0.0899, -0.0697, -0.0506, -0.0426, -0.1033, -0.0663,\n",
      "        -0.0284,  0.0762,  0.0550,  0.0492, -0.0169, -0.0705,  0.0062,  0.0039,\n",
      "        -0.0764,  0.0688, -0.0799,  0.0440], device='cuda:0')\n",
      "bias_hh_l0\n",
      "tensor([ 0.0780, -0.0220, -0.0328,  0.0320,  0.0151, -0.0860,  0.0567,  0.0668,\n",
      "         0.1011,  0.0412, -0.0727, -0.0556, -0.0545, -0.0491, -0.0220, -0.0551,\n",
      "         0.0400, -0.0831, -0.1060,  0.0521,  0.0686,  0.1019,  0.0633,  0.0219,\n",
      "         0.0310, -0.0883,  0.0541,  0.0714, -0.0706,  0.0204, -0.0187,  0.0064,\n",
      "         0.0407, -0.0594,  0.0012,  0.0754,  0.0340,  0.0415,  0.0702, -0.0703,\n",
      "         0.0810, -0.0325, -0.0099, -0.0586, -0.0669, -0.0349, -0.0687, -0.0479,\n",
      "        -0.0575, -0.0219,  0.0798, -0.0607, -0.0143,  0.0919, -0.0216,  0.0124,\n",
      "         0.0685,  0.0412,  0.0530,  0.0324, -0.0712,  0.0408,  0.0187,  0.0231,\n",
      "        -0.0943,  0.0050,  0.0754,  0.0662, -0.0457, -0.0708, -0.0056, -0.0155,\n",
      "         0.0488, -0.0651, -0.0302,  0.0154, -0.0608,  0.0194,  0.0743,  0.0071,\n",
      "        -0.0531,  0.0217, -0.0899, -0.0697, -0.0506, -0.0426, -0.1033, -0.0663,\n",
      "        -0.0284,  0.0762,  0.0550,  0.0492, -0.0169, -0.0705,  0.0062,  0.0039,\n",
      "        -0.0764,  0.0688, -0.0799,  0.0440], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, p in rnn.named_parameters():\n",
    "    print(name)\n",
    "    print(p.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = torch.zeros_like(Wxh), torch.zeros_like(Whh), torch.zeros_like(Why)\n",
    "mbh, mby = torch.zeros_like(bh), torch.zeros_like(by) # memory variables for Adagrad\n",
    "#smooth_loss = -torch.log(1.0/27.0)*seq_length # loss at iteration 0\n",
    "#smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "#print('iter %d, loss: %f' % (n, smooth_loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(bh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward \n",
    "def forward(x, y, h_prev):\n",
    "    xs, hs, ys, ps = {},{},{},{}\n",
    "    loss = 0.0\n",
    "    hs[-1] = h_prev.clone()\n",
    "    for t in range(len(x)):\n",
    "        xs[t] = x[t].unsqueeze(0)\n",
    "        hs[t] = torch.tanh(xs[t] @ Wxh.T + bx + hs[t-1] @ Whh.T  + bh)\n",
    "        ys[t] = hs[t] @ Why.T + by\n",
    "        ps[t] = torch.exp(ys[t]) / torch.sum(torch.exp(ys[t])) \n",
    "        loss += -torch.log(ps[t][0,y[t].argmax().item()])\n",
    "    return loss, ps, ys, hs, xs\n",
    "\n",
    "def backward(ps, x, y, hs, xs):\n",
    "    #backward in time\n",
    "    dWxh, dWhh, dWhy = torch.zeros_like(Wxh), torch.zeros_like(Whh), torch.zeros_like(Why), \n",
    "    dbh, dby = torch.zeros_like(bh), torch.zeros_like(by)\n",
    "    dhnext = torch.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(x))):\n",
    "        dy = ps[t].clone()\n",
    "        dy[0, y[t].argmax().item()] -= 1\n",
    "        dWhy +=  dy.T @ hs[t] \n",
    "        dby += dy.sum(0)\n",
    "        # ------ below not tested ----\n",
    "        dh = dy @ Why + dhnext\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "        dbh = dhraw.sum(0)\n",
    "        dWxh +=  dhraw.T @ xs[t]  \n",
    "        dWhh += hs[t-1] @ dhraw.T  \n",
    "        dhnext = dhraw @ Whh.T  \n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, hs[len(x)-1]]:\n",
    "        torch.clip(dparam, -5, 5, out=dparam)\n",
    "    return dWxh, dWhh, dWhy, dbh, dby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prev = torch.zeros(hidden_size).to(\"cuda\")\n",
    "lossme, ps, ys, hs, xs = forward(x_tr[0].unsqueeze(0), y_tr[0].unsqueeze(0), h_prev)\n",
    "dWxh, dWhh, dWhy, dbh, dby = backward(ps, x_tr[0].unsqueeze(0),y_tr[0].unsqueeze(0), hs, xs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.1033, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "#cmp(\"Wxh\", dWxh, linear1.weight_hh_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 8.940229\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m [Wxh, Whh, Why, bh, by]:\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=16'>17</a>\u001b[0m         param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=17'>18</a>\u001b[0m     dWxh, dWhh, dWhy, dbh, dby \u001b[39m=\u001b[39m backward(ps, inputs,targets, hs, xs) \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=20'>21</a>\u001b[0m \u001b[39m# perform parameter update with Adagrad\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m param, dparam, mem \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m([Wxh, Whh, Why, bh, by], \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=22'>23</a>\u001b[0m                                 [dWxh, dWhh, dWhy, dbh, dby], \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=23'>24</a>\u001b[0m                                 [mWxh, mWhh, mWhy, mbh, mby]):\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 22\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(ps, x, y, hs, xs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=23'>24</a>\u001b[0m \u001b[39m# ------ below not tested ----\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m dh \u001b[39m=\u001b[39m dy \u001b[39m@\u001b[39m Why \u001b[39m+\u001b[39m dhnext\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m dhraw \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m hs[t] \u001b[39m*\u001b[39;49m hs[t]) \u001b[39m*\u001b[39m dh\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=26'>27</a>\u001b[0m dbh \u001b[39m=\u001b[39m dhraw\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X41sdW50aXRsZWQ%3D?line=27'>28</a>\u001b[0m dWxh \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  dhraw\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m xs[t]  \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = torch.zeros_like(Wxh), torch.zeros_like(Whh), torch.zeros_like(Why)\n",
    "mbh, mby = torch.zeros_like(bh), torch.zeros_like(by) \n",
    "\n",
    "epoch=1000\n",
    "for i in range(epoch):\n",
    "    h_prev = torch.zeros(1, hidden_size).to(\"cuda\")\n",
    "    for p in range(0, x_tr.shape[0], seq_length):\n",
    "        # forward\n",
    "        inputs = x_tr[p:p+seq_length]\n",
    "        targets= y_tr[p:p+seq_length]\n",
    "\n",
    "        loss, ps,ys, hs, xs = forward(inputs, targets, h_prev)\n",
    "\n",
    "        # backward\n",
    "        for param in [Wxh, Whh, Why, bh, by]:\n",
    "            param.grad = None\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backward(ps, inputs,targets, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -lr* dparam / torch.sqrt(mem + 1e-8) # adagrad update      \n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(test_char, length):\n",
    "    x = torch.zeros((1, 27)) \n",
    "    x[0,stoi[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = torch.zeros((1,hidden_size))\n",
    "    wxh = Wxh.clone().cpu()\n",
    "    whh = Whh.clone().cpu()\n",
    "    why = Why.clone().cpu()\n",
    "    b = bh.clone().cpu()\n",
    "    bb = by.clone().cpu()\n",
    "\n",
    "    for t in range(length):\n",
    "        h = torch.tanh(x @ wxh.T + h @ whh.T  + b)\n",
    "        y = h @ why.T + bb\n",
    "        p = torch.exp(y) / torch.sum(torch.exp(y)) \n",
    "        ix = np.random.choice(range(27), p=p.numpy().ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = torch.zeros((1,27)) # init\n",
    "        x[0,ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = ''.join(itos[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " rejauaaole \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('a',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ior \n",
      "----\n",
      "----\n",
      " elie \n",
      "----\n",
      "----\n",
      " elyna \n",
      "----\n",
      "----\n",
      " arisel \n",
      "----\n",
      "----\n",
      " encered \n",
      "----\n",
      "----\n",
      " eelliann \n",
      "----\n",
      "----\n",
      " emaleelan \n",
      "----\n",
      "----\n",
      " nna \n",
      "----\n",
      "----\n",
      " nnav \n",
      "----\n",
      "----\n",
      " ionsa \n",
      "----\n",
      "----\n",
      " ishtia \n",
      "----\n",
      "----\n",
      " ndieyan \n",
      "----\n",
      "----\n",
      " ykataiqu \n",
      "----\n",
      "----\n",
      " saiqusemy \n",
      "----\n",
      "----\n",
      " emy \n",
      "----\n",
      "----\n",
      " esad \n",
      "----\n",
      "----\n",
      " esstl \n",
      "----\n",
      "----\n",
      " esayna \n",
      "----\n",
      "----\n",
      " etleval \n",
      "----\n",
      "----\n",
      " esslayre \n",
      "----\n",
      "----\n",
      " essllyana \n",
      "----\n",
      "----\n",
      " eyl \n",
      "----\n",
      "----\n",
      " harl \n",
      "----\n",
      "----\n",
      " aszal \n",
      "----\n",
      "----\n",
      " haista \n",
      "----\n",
      "----\n",
      " hancera \n",
      "----\n",
      "----\n",
      " hriyeras \n",
      "----\n",
      "----\n",
      " hanaskyle \n",
      "----\n",
      "----\n",
      " eny \n",
      "----\n",
      "----\n",
      " anda \n",
      "----\n",
      "----\n",
      " yiaha \n",
      "----\n",
      "----\n",
      " aishio \n",
      "----\n",
      "----\n",
      " ancersi \n",
      "----\n",
      "----\n",
      " ayeiyorl \n",
      "----\n",
      "----\n",
      " ielledgan \n",
      "----\n",
      "----\n",
      " myi \n",
      "----\n",
      "----\n",
      " liai \n",
      "----\n",
      "----\n",
      " lysin \n",
      "----\n",
      "----\n",
      " idalda \n",
      "----\n",
      "----\n",
      " llukasi \n",
      "----\n",
      "----\n",
      " laencers \n",
      "----\n",
      "----\n",
      " anasslayo \n",
      "----\n",
      "----\n",
      " rei \n",
      "----\n",
      "----\n",
      " rely \n",
      "----\n",
      "----\n",
      " ranna \n",
      "----\n",
      "----\n",
      " reyans \n",
      "----\n",
      "----\n",
      " oparqus \n",
      "----\n",
      "----\n",
      " aidiland \n",
      "----\n",
      "----\n",
      " rannavish \n",
      "----\n",
      "----\n",
      " aro \n",
      "----\n",
      "----\n",
      " arab \n",
      "----\n",
      "----\n",
      " arsey \n",
      "----\n",
      "----\n",
      " elinab \n",
      "----\n",
      "----\n",
      " encegan \n",
      "----\n",
      "----\n",
      " emyeneva \n",
      "----\n",
      "----\n",
      " enckeysan \n",
      "----\n",
      "----\n",
      " aid \n",
      "----\n",
      "----\n",
      " idal \n",
      "----\n",
      "----\n",
      " atedg \n",
      "----\n",
      "----\n",
      " idylan \n",
      "----\n",
      "----\n",
      " eyoryla \n",
      "----\n",
      "----\n",
      " eyamoris \n",
      "----\n",
      "----\n",
      " anysrlian \n",
      "----\n",
      "----\n",
      " eis \n",
      "----\n",
      "----\n",
      " llyd \n",
      "----\n",
      "----\n",
      " lansa \n",
      "----\n",
      "----\n",
      " siahel \n",
      "----\n",
      "----\n",
      " lannass \n",
      "----\n",
      "----\n",
      " sielinas \n",
      "----\n",
      "----\n",
      " landanasa \n",
      "----\n",
      "----\n",
      " ayo \n",
      "----\n",
      "----\n",
      " anna \n",
      "----\n",
      "----\n",
      " arist \n",
      "----\n",
      "----\n",
      " adiorl \n",
      "----\n",
      "----\n",
      " adencee \n",
      "----\n",
      "----\n",
      " adiorlin \n",
      "----\n",
      "----\n",
      " aidanceri \n",
      "----\n",
      "----\n",
      " eiy \n",
      "----\n",
      "----\n",
      " yadr \n",
      "----\n",
      "----\n",
      " htiay \n",
      "----\n",
      "----\n",
      " aiquen \n",
      "----\n",
      "----\n",
      " etyandi \n",
      "----\n",
      "----\n",
      " eelaynev \n",
      "----\n",
      "----\n",
      " asaylandr \n",
      "----\n",
      "----\n",
      " iat \n",
      "----\n",
      "----\n",
      " eysa \n",
      "----\n",
      "----\n",
      " laszy \n",
      "----\n",
      "----\n",
      " eleell \n",
      "----\n",
      "----\n",
      " eykesay \n",
      "----\n",
      "----\n",
      " iannavie \n",
      "----\n",
      "----\n",
      " eelysiell \n",
      "----\n",
      "----\n",
      " oer \n",
      "----\n",
      "----\n",
      " ikhz \n",
      "----\n",
      "----\n",
      " akyre \n",
      "----\n",
      "----\n",
      " yersal \n",
      "----\n",
      "----\n",
      " ikaskyl \n",
      "----\n",
      "----\n",
      " adlansad \n",
      "----\n",
      "----\n",
      " oerleylav \n",
      "----\n",
      "----\n",
      " avi \n",
      "----\n",
      "----\n",
      " ssai \n",
      "----\n",
      "----\n",
      " navie \n",
      "----\n",
      "----\n",
      " aviria \n",
      "----\n",
      "----\n",
      " avirian \n",
      "----\n",
      "----\n",
      " asamoeli \n",
      "----\n",
      "----\n",
      " asayontem \n",
      "----\n",
      "----\n",
      " pha \n",
      "----\n",
      "----\n",
      " navi \n",
      "----\n",
      "----\n",
      " shior \n",
      "----\n",
      "----\n",
      " eliann \n",
      "----\n",
      "----\n",
      " shaisil \n",
      "----\n",
      "----\n",
      " rselanys \n",
      "----\n",
      "----\n",
      " slahnavil \n",
      "----\n",
      "----\n",
      " ait \n",
      "----\n",
      "----\n",
      " hron \n",
      "----\n",
      "----\n",
      " auina \n",
      "----\n",
      "----\n",
      " aukian \n",
      "----\n",
      "----\n",
      " henyana \n",
      "----\n",
      "----\n",
      " hersslay \n",
      "----\n",
      "----\n",
      " arliahnev \n",
      "----\n",
      "----\n",
      " uen \n",
      "----\n",
      "----\n",
      " uesl \n",
      "----\n",
      "----\n",
      " uensa \n",
      "----\n",
      "----\n",
      " uensad \n",
      "----\n",
      "----\n",
      " usayosh \n",
      "----\n",
      "----\n",
      " uenavahl \n",
      "----\n",
      "----\n",
      " uenaszceg \n",
      "----\n",
      "----\n",
      " lin \n",
      "----\n",
      "----\n",
      " leyl \n",
      "----\n",
      "----\n",
      " essel \n",
      "----\n",
      "----\n",
      " ellega \n",
      "----\n",
      "----\n",
      " ldwelli \n",
      "----\n",
      "----\n",
      " essyonna \n",
      "----\n",
      "----\n",
      " iasiaique \n",
      "----\n",
      "----\n",
      " ayd \n",
      "----\n",
      "----\n",
      " emma \n",
      "----\n",
      "----\n",
      " aiqus \n",
      "----\n",
      "----\n",
      " aioshn \n",
      "----\n",
      "----\n",
      " ellynav \n",
      "----\n",
      "----\n",
      " eyllesly \n",
      "----\n",
      "----\n",
      " anassithz \n",
      "----\n",
      "----\n",
      " aik \n",
      "----\n",
      "----\n",
      " aisi \n",
      "----\n",
      "----\n",
      " aisil \n",
      "----\n",
      "----\n",
      " aisiel \n",
      "----\n",
      "----\n",
      " aszyali \n",
      "----\n",
      "----\n",
      " aiorleyn \n",
      "----\n",
      "----\n",
      " hzikaliah \n",
      "----\n",
      "----\n",
      " ket \n",
      "----\n",
      "----\n",
      " syav \n",
      "----\n",
      "----\n",
      " lukya \n",
      "----\n",
      "----\n",
      " iyneva \n",
      "----\n",
      "----\n",
      " szyliya \n",
      "----\n",
      "----\n",
      " khandrie \n",
      "----\n",
      "----\n",
      " shilyyedg \n",
      "----\n",
      "----\n",
      " ish \n",
      "----\n",
      "----\n",
      " iche \n",
      "----\n",
      "----\n",
      " ivira \n",
      "----\n",
      "----\n",
      " ivisha \n",
      "----\n",
      "----\n",
      " ahaiely \n",
      "----\n",
      "----\n",
      " ivivahnz \n",
      "----\n",
      "----\n",
      " ahiahrima \n",
      "----\n",
      "----\n",
      " ell \n",
      "----\n",
      "----\n",
      " eled \n",
      "----\n",
      "----\n",
      " elynn \n",
      "----\n",
      "----\n",
      " ennavi \n",
      "----\n",
      "----\n",
      " elynasz \n",
      "----\n",
      "----\n",
      " ellydanz \n",
      "----\n",
      "----\n",
      " lielakhae \n",
      "----\n",
      "----\n",
      " eth \n",
      "----\n",
      "----\n",
      " eele \n",
      "----\n",
      "----\n",
      " ianda \n",
      "----\n",
      "----\n",
      " ellian \n",
      "----\n",
      "----\n",
      " elannte \n",
      "----\n",
      "----\n",
      " ellayonn \n",
      "----\n",
      "----\n",
      " eleyensky \n",
      "----\n",
      "----\n",
      " nan \n",
      "----\n",
      "----\n",
      " onna \n",
      "----\n",
      "----\n",
      " navir \n",
      "----\n",
      "----\n",
      " ondana \n",
      "----\n",
      "----\n",
      " ndellya \n",
      "----\n",
      "----\n",
      " labendre \n",
      "----\n",
      "----\n",
      " onsoshard \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(26):\n",
    "    for j in range(3,10):\n",
    "        predict(itos[i],j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6f7f28239ca37c9432657f7c668b4855da73a6d9d20a0b43e9bc696665eb3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
